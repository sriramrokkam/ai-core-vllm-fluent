apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: vllm
  annotations:
    scenarios.ai.sap.com/description: "Run a vLLM server on SAP AI Core"
    scenarios.ai.sap.com/name: "vllm"
    executables.ai.sap.com/description: "Run a LocalAI server on SAP AI Core"
    executables.ai.sap.com/name: "vllm"
  labels:
    scenarios.ai.sap.com/id: "vllm"
    ai.sap.com/version: "0.0.1"
spec:
  inputs:
    parameters:
      - name: modelName # placeholder name
        default: "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
        type: string # required for every parameters
        description: "--model: Name or path of the huggingface model to use. https://docs.vllm.ai/en/latest/models/supported_models.html/ for supported models. The same model name is used with SAP Generative AI Hub SDK"
      - name: dataType # Data Type
        default: "half"
        type: string # required for every parameters
        description: "--dtype: Data type for model weights and activations. https://docs.vllm.ai/en/latest/models/engine_args.html"
      - name: gpuMemoryUtilization # GPU Memory Utilization
        default: "0.9"
        type: string # required for every parameters
        description: "The fraction(0~1) of GPU memory to be used for the model executor. https://docs.vllm.ai/en/latest/models/engine_args.html"
      - name: maxTokenLen # placeholder name
        default: "2048"
        type: string # required for every parameters
        description: "--max-model-len: Model context length. https://docs.vllm.ai/en/latest/models/engine_args.html"
      - name: maxNumBatchedTokens # placeholder name
        default: "2048"
        type: string # required for every parameters
        description: "--max-num-batched-tokens: Maximum number of batched tokens per iteration. https://docs.vllm.ai/en/latest/models/engine_args.html"
      - name: maxNumSeqs # placeholder name
        default: "2048"
        type: string # required for every parameters
        description: "--max-num-seqs: Maximum number of sequences per iteration. https://docs.vllm.ai/en/latest/models/engine_args.html"
      - name: quantization # placeholder name
        default: "None"
        type: string # required for every parameters
        description: "--quantization: Method used to quantize the weights. https://docs.vllm.ai/en/latest/models/engine_args.html"
      - name: resourcePlan
        type: "string"
        default: "infer.s"
        description: "Resource Plan of SAP AI Core. Supported: infer.s, infer.m, infer.l, train.l"
  template:
    apiVersion: "serving.kserve.io/v1beta1"
    metadata:
      annotations: |
        autoscaling.knative.dev/metric: concurrency
        autoscaling.knative.dev/target: 1
        autoscaling.knative.dev/targetBurstCapacity: 0
      labels: |
        ai.sap.com/resourcePlan: "{{inputs.parameters.resourcePlan}}"
    spec: |
      predictor:
        imagePullSecrets:
        - name: "vllm-poc-artifactory"
        minReplicas: 1
        maxReplicas: 1
        containers:
        - name: kserve-container
          image: "vllm-poc.common.repositories.cloud.sap/vllm-openai:latest"
          ports:
            - containerPort: 8000
              protocol: TCP
          command: ["/app/start.sh"]
          args:
            - "--model"
            - "{{inputs.parameters.modelName}}"
            - "--dtype"
            - "{{inputs.parameters.dataType}}"
            - "--gpu-memory-utilization"
            - "{{inputs.parameters.gpuMemoryUtilization}}"
            - "--enforce-eager"
            - "--max-model-len"
            - "{{inputs.parameters.maxTokenLen}}"
            - "--max-num-batched-tokens"
            - "{{inputs.parameters.maxNumBatchedTokens}}"
            - "--max-num-seqs"
            - "{{inputs.parameters.maxNumSeqs}}"
            - "--quantization"
            - "{{inputs.parameters.quantization}}"
          env:
            - name: MODEL
              value: "{{inputs.parameters.modelName}}"
            # AI Core metadata for log enrichment
            - name: AICORE_DEPLOYMENT_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: AICORE_RESOURCE_GROUP
              value: "default"
            - name: AICORE_SCENARIO_ID
              value: "vllm"
            - name: AICORE_EXECUTION_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            # Cloud Logging configuration
            - name: CLOUD_LOGGING_HOST
              valueFrom:
                secretKeyRef:
                  name: cloud-logging-credentials
                  key: host
            - name: CLOUD_LOGGING_USER
              valueFrom:
                secretKeyRef:
                  name: cloud-logging-credentials
                  key: user
            - name: CLOUD_LOGGING_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cloud-logging-credentials
                  key: password
